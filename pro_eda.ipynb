{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85723,"databundleVersionId":10652996,"sourceType":"competition"},{"sourceId":3325325,"sourceType":"datasetVersion","datasetId":2007861}],"dockerImageVersionId":30235,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Forecasting with Linear Regression**","metadata":{}},{"cell_type":"markdown","source":"### **Contents**\n\n- **EDA** - A brief EDA, showing the essentials\n- **Aggregating Categorical Variables** - A continuation of the EDA, showing that we should be able to forecast the aggregated time series (daily total sales) and then disaggregate the forecasts based on historical proportions and other data without penalising performance.\n- **Total Sales Forecast** - Forecast the total number of sales across all categorical variables using Linear Regression for 2017, 2018 and 2019.\n- **Product Sales Ratio Forecast** - Forecast the ratio of sales between products for 2017, 2018 and 2019.\n- **Dissagregating Total Sales Forecast** - Disagreggate the Total Sales forecasts, to get the forecast for each categorical variable.","metadata":{}},{"cell_type":"markdown","source":"### **References**","metadata":{}},{"cell_type":"markdown","source":"This work is based off my notebook from Season 2, on a similar competition:\n\n- https://www.kaggle.com/code/cabaxiom/tps-sep-22-eda-and-linear-regression-baseline ","metadata":{}},{"cell_type":"markdown","source":"# **Preliminaries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge\n\nsns.set_style('darkgrid')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-04T21:24:57.452766Z","iopub.execute_input":"2025-01-04T21:24:57.453178Z","iopub.status.idle":"2025-01-04T21:24:58.333994Z","shell.execute_reply.started":"2025-01-04T21:24:57.453148Z","shell.execute_reply":"2025-01-04T21:24:58.332779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/train.csv\", parse_dates=[\"date\"])\noriginal_train_df = train_df.copy()\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e1/test.csv\", parse_dates=[\"date\"])","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:58.33613Z","iopub.execute_input":"2025-01-04T21:24:58.336459Z","iopub.status.idle":"2025-01-04T21:24:58.929329Z","shell.execute_reply.started":"2025-01-04T21:24:58.336432Z","shell.execute_reply":"2025-01-04T21:24:58.928169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"markdown","source":"## **Categorical variables**","metadata":{}},{"cell_type":"code","source":"display(train_df.head(3))\ndisplay(test_df.head(3))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:58.93044Z","iopub.execute_input":"2025-01-04T21:24:58.930762Z","iopub.status.idle":"2025-01-04T21:24:58.961568Z","shell.execute_reply.started":"2025-01-04T21:24:58.930732Z","shell.execute_reply":"2025-01-04T21:24:58.960195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- There are 3 categorical columns that together describe a univariate time series. Country, Store and Product.","metadata":{}},{"cell_type":"markdown","source":"Lets see which countries, stores and products we have data for:","metadata":{}},{"cell_type":"code","source":"def get_val_counts(df, column_name, sort_by_column_name=False):\n    value_count = df[column_name].value_counts().reset_index().rename(columns={column_name:\"Value Count\",\"index\":column_name}).set_index(column_name)\n    value_count[\"Percentage\"] = df[column_name].value_counts(normalize=True)*100\n    value_count = value_count.reset_index()\n    if sort_by_column_name:\n        value_count = value_count.sort_values(column_name)\n    return value_count\n\ndef plot_value_counts_pie(df, column_name, sort_by_column_name=False):\n    val_count_df = get_val_counts(df, column_name, sort_by_column_name)\n    val_count.set_index(column_name).plot.pie(y=\"Value Count\", figsize=(5,5), legend=False, ylabel=\"\");\n\ndef plot_value_counts_bar(df, column_name, sort_by_column_name = False):\n    val_count_df = get_val_counts(df, column_name, sort_by_column_name)\n    f,ax = plt.subplots(figsize=(12,6))\n    sns.barplot(data = val_count_df, y=\"Value Count\", x=column_name )\n\n    for index, row in val_count_df.iterrows():\n        count = row[\"Value Count\"]\n        percentage = row[\"Percentage\"]\n        ax.text(\n            x=index, \n            y=row[\"Value Count\"] + max(val_count_df[\"Value Count\"])*0.02,  # Adjust position slightly above the bar\n            s=f'{count} ({percentage:.2f}%)', \n            ha='center', \n            va='bottom'\n        )","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:58.964176Z","iopub.execute_input":"2025-01-04T21:24:58.964535Z","iopub.status.idle":"2025-01-04T21:24:58.97584Z","shell.execute_reply.started":"2025-01-04T21:24:58.964505Z","shell.execute_reply":"2025-01-04T21:24:58.974495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_value_counts_bar(train_df, \"country\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:24:58.977597Z","iopub.execute_input":"2025-01-04T21:24:58.978057Z","iopub.status.idle":"2025-01-04T21:24:59.338153Z","shell.execute_reply.started":"2025-01-04T21:24:58.978005Z","shell.execute_reply":"2025-01-04T21:24:59.336793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_value_counts_bar(train_df, \"store\")","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:59.340029Z","iopub.execute_input":"2025-01-04T21:24:59.340449Z","iopub.status.idle":"2025-01-04T21:24:59.562413Z","shell.execute_reply.started":"2025-01-04T21:24:59.340417Z","shell.execute_reply":"2025-01-04T21:24:59.561081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_value_counts_bar(train_df, \"product\")","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:59.564382Z","iopub.execute_input":"2025-01-04T21:24:59.564861Z","iopub.status.idle":"2025-01-04T21:24:59.858521Z","shell.execute_reply.started":"2025-01-04T21:24:59.564819Z","shell.execute_reply":"2025-01-04T21:24:59.857266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- We have 6 Countries, all occuring in the dataset the same number of time (equal proportions).\n- We have 3 Stores, all occuring in the dataset the same number of time (equal proportions)\n- We have 5 products, all occuring in the dataset the same number of time (equal proportions)","metadata":{}},{"cell_type":"markdown","source":"Look at the number of rows for each country, store and product:","metadata":{}},{"cell_type":"code","source":"counts = train_df.groupby([\"country\",\"store\",\"product\"])[\"id\"].count().rename(\"num_rows\").reset_index()\ncounts_val_counts = counts[\"num_rows\"].value_counts().rename(\"Count\").reset_index().rename(columns={\"index\": \"length\"})\ndisplay(counts_val_counts.head(10))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:24:59.859676Z","iopub.execute_input":"2025-01-04T21:24:59.859983Z","iopub.status.idle":"2025-01-04T21:24:59.958785Z","shell.execute_reply.started":"2025-01-04T21:24:59.859957Z","shell.execute_reply":"2025-01-04T21:24:59.957402Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In total we have **90 univariate time series** all of **length 2557**\n\nHowever although we have 2557 rows for every single country, product and store, we may have missing values in the number of sales:","metadata":{}},{"cell_type":"code","source":"print(f\"Number of missing num_sold rows: {train_df['num_sold'].isna().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:24:59.960218Z","iopub.execute_input":"2025-01-04T21:24:59.96054Z","iopub.status.idle":"2025-01-04T21:24:59.968436Z","shell.execute_reply.started":"2025-01-04T21:24:59.960511Z","shell.execute_reply":"2025-01-04T21:24:59.967032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"counts = train_df.groupby([\"country\",\"store\",\"product\"])[\"num_sold\"].count().rename(\"num_rows\")\nmissing_data = counts.loc[counts != 2557]\nmissing_data_df = missing_data.reset_index()\nmissing_data_df[\"num_missing_rows\"] = 2557 - missing_data_df[\"num_rows\"]\nmissing_data_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:24:59.974089Z","iopub.execute_input":"2025-01-04T21:24:59.974511Z","iopub.status.idle":"2025-01-04T21:25:00.078509Z","shell.execute_reply.started":"2025-01-04T21:24:59.974479Z","shell.execute_reply":"2025-01-04T21:25:00.077153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Obseervations:**\n- In total 9 of the 90 time series (10%) have atleast some missing some data.\n- 2 of the time series are completely missing data *Canada, Discount Stickers, Holographic Goose* and *Kenya, Discount Stickers, Holographic Goose*\n- 2 of the time series are only missing a single day of data *Canada, Discount Stickers, Kerneler* and *Kenya, Discount Stickers, Kernerler Dark Mode*\n\nLets take a closer look at when the missing values occur in each of these time series:","metadata":{}},{"cell_type":"code","source":"f,axs = plt.subplots(9,1, figsize=(20,50))\nfor i, (country, store, product) in enumerate(missing_data.index):\n    plot_df = train_df.loc[(train_df[\"country\"] == country) & (train_df[\"store\"] == store) & (train_df[\"product\"] == product)]\n    missing_vals = plot_df.loc[plot_df[\"num_sold\"].isna()]\n    sns.lineplot(data=plot_df, x=\"date\", y=\"num_sold\", ax=axs[i])\n    for missing_date in missing_vals[\"date\"]:\n        axs[i].axvline(missing_date, color='red',  linestyle='-', linewidth=1, alpha=0.2)\n    axs[i].set_title(f\"{country} - {store} - {product}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:00.080848Z","iopub.execute_input":"2025-01-04T21:25:00.08142Z","iopub.status.idle":"2025-01-04T21:25:21.381219Z","shell.execute_reply.started":"2025-01-04T21:25:00.081375Z","shell.execute_reply":"2025-01-04T21:25:21.379878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- The missing data is not missing completely randomly (with respect to time), some periods contain more missing data than others.\n- It looks data is missing when the value for num_sold < 200 for Canada and < 5 for Kenya (for most the time series). We could impute based on that assumption, but I've used a different method.\n","metadata":{}},{"cell_type":"markdown","source":"## **Time series**","metadata":{}},{"cell_type":"code","source":"print(\"Train - Earliest date:\", train_df[\"date\"].min())\nprint(\"Train - Latest date:\", train_df[\"date\"].max())\n\nprint(\"Test - Earliest date:\", test_df[\"date\"].min())\nprint(\"Test - Latest date:\", test_df[\"date\"].max())","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:21.382605Z","iopub.execute_input":"2025-01-04T21:25:21.382987Z","iopub.status.idle":"2025-01-04T21:25:21.396165Z","shell.execute_reply.started":"2025-01-04T21:25:21.382956Z","shell.execute_reply":"2025-01-04T21:25:21.394397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- We have **7 years** of data **from 2010-01-01 to 2016-12-31** to train occuring at **daily frequency**.\n- We are required to forecast 3 year of data, **from 2017-01-01 to 2019-12-31**","metadata":{}},{"cell_type":"markdown","source":"Lets take a look at the overall trends for each time series:","metadata":{}},{"cell_type":"code","source":"weekly_df = train_df.groupby([\"country\",\"store\", \"product\", pd.Grouper(key=\"date\", freq=\"W\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()\nmonthly_df = train_df.groupby([\"country\",\"store\", \"product\", pd.Grouper(key=\"date\", freq=\"MS\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:21.397624Z","iopub.execute_input":"2025-01-04T21:25:21.397975Z","iopub.status.idle":"2025-01-04T21:25:21.573547Z","shell.execute_reply.started":"2025-01-04T21:25:21.39794Z","shell.execute_reply":"2025-01-04T21:25:21.572213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_all(df):\n    f,axes = plt.subplots(3,2,figsize=(25,25), sharex = True, sharey=True)\n    f.tight_layout()\n    for n,prod in enumerate(df[\"product\"].unique()):\n        plot_df = df.loc[df[\"product\"] == prod]\n        sns.lineplot(data=plot_df, x=\"date\", y=\"num_sold\", hue=\"country\", style=\"store\",ax=axes[n//2,n%2])\n        axes[n//2,n%2].set_title(\"Product: \"+str(prod))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:21.575245Z","iopub.execute_input":"2025-01-04T21:25:21.575587Z","iopub.status.idle":"2025-01-04T21:25:21.58351Z","shell.execute_reply.started":"2025-01-04T21:25:21.575557Z","shell.execute_reply":"2025-01-04T21:25:21.582123Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_all(monthly_df)\n#plot_all(weekly_df)","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:21.585436Z","iopub.execute_input":"2025-01-04T21:25:21.58592Z","iopub.status.idle":"2025-01-04T21:25:25.053636Z","shell.execute_reply.started":"2025-01-04T21:25:21.585851Z","shell.execute_reply":"2025-01-04T21:25:25.051867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Aggregating Time Series**","metadata":{}},{"cell_type":"markdown","source":"The main theme of this notebook is to show that its a good idea to aggregate the time series across each of the three categorical variables: Store, Country and Product.","metadata":{}},{"cell_type":"markdown","source":"## **Country**","metadata":{}},{"cell_type":"markdown","source":"First we show that its a good idea to aggregate **countries** when we make the forecast.\n\nTo do this we need to show that the proportion of total sales for each country remains constant, regardless of time.\n\nIn the graph below, we are looking for straight lines for each country:\n\n","metadata":{}},{"cell_type":"code","source":"country_weights = train_df.groupby(\"country\")[\"num_sold\"].sum()/train_df[\"num_sold\"].sum()\n\ncountry_ratio_over_time = (train_df.groupby([\"date\",\"country\"])[\"num_sold\"].sum() / train_df.groupby([\"date\"])[\"num_sold\"].sum()).reset_index()\nf,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = country_ratio_over_time, x=\"date\", y=\"num_sold\", hue=\"country\");\nax.set_ylabel(\"Proportion of sales\");","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:25.055143Z","iopub.execute_input":"2025-01-04T21:25:25.055475Z","iopub.status.idle":"2025-01-04T21:25:26.134553Z","shell.execute_reply.started":"2025-01-04T21:25:25.055446Z","shell.execute_reply":"2025-01-04T21:25:26.13308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- The lines are **not** perflectly straight, meaning a single constant does not explain the proportion of sales regardless of time.\n- The lines for each country do seem to have rises and falls each year (noteably exactly at the year markings) something artificially strange is going on here.","metadata":{}},{"cell_type":"markdown","source":"The link seems to be GDP per captia, credit to [@siukeitin](https://www.kaggle.com/siukeitin) for discovering this in this discussion thread https://www.kaggle.com/competitions/playground-series-s5e1/discussion/554349","metadata":{}},{"cell_type":"code","source":"gdp_per_capita_df = pd.read_csv(\"/kaggle/input/world-gdpgdp-gdp-per-capita-and-annual-growths/gdp_per_capita.csv\")\n\nyears =  [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\ngdp_per_capita_filtered_df = gdp_per_capita_df.loc[gdp_per_capita_df[\"Country Name\"].isin(train_df[\"country\"].unique()), [\"Country Name\"] + years].set_index(\"Country Name\")\ngdp_per_capita_filtered_df[\"2010_ratio\"] = gdp_per_capita_filtered_df[\"2010\"] / gdp_per_capita_filtered_df.sum()[\"2010\"]\nfor year in years:\n    gdp_per_capita_filtered_df[f\"{year}_ratio\"] = gdp_per_capita_filtered_df[year] / gdp_per_capita_filtered_df.sum()[year]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_df[[i+\"_ratio\" for i in years]]\ngdp_per_capita_filtered_ratios_df.columns = [int(i) for i in years]\ngdp_per_capita_filtered_ratios_df = gdp_per_capita_filtered_ratios_df.unstack().reset_index().rename(columns = {\"level_0\": \"year\", 0: \"ratio\", \"Country Name\": \"country\"})\ngdp_per_capita_filtered_ratios_df['year'] = pd.to_datetime(gdp_per_capita_filtered_ratios_df['year'], format='%Y')\n\n# For plotting purposes\ngdp_per_capita_filtered_ratios_df_2 = gdp_per_capita_filtered_ratios_df.copy()\ngdp_per_capita_filtered_ratios_df_2[\"year\"] = pd.to_datetime(gdp_per_capita_filtered_ratios_df_2['year'].astype(str)) + pd.offsets.YearEnd(1)\ngdp_per_capita_filtered_ratios_df = pd.concat([gdp_per_capita_filtered_ratios_df, gdp_per_capita_filtered_ratios_df_2]).reset_index()\n\nf,ax = plt.subplots(figsize=(20,15))\nsns.lineplot(data = country_ratio_over_time, x=\"date\", y=\"num_sold\", hue=\"country\");\nsns.lineplot(data = gdp_per_capita_filtered_ratios_df, x=\"year\", y = \"ratio\", hue=\"country\", palette = [\"black\"]*6, legend = False)\nax.set_ylabel(\"Proportion of sales\");","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:26.136287Z","iopub.execute_input":"2025-01-04T21:25:26.136667Z","iopub.status.idle":"2025-01-04T21:25:27.308836Z","shell.execute_reply.started":"2025-01-04T21:25:26.136633Z","shell.execute_reply":"2025-01-04T21:25:27.307446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- The black line shows the ratio of GDP per captia for each year for that country compared to the sum of GDP per capita for all the other countries.\n- Note that Canada and Kenya do not perfectly allign to these ratios, likely because of missing values, this is fine.\n- There might be some slight non-random noise here, so perhaps this method isn't quite perfect?\n\n**Insight:**\n- This means we can predict the proportion of sales between each country for each year that we have to forecast for, by considering the annual GDP per capita. This means we can aggregate the number of sales across countries for each product and store when making the forecast and then disagregate using the known annual GDP per capita ratios for the years we are predicting for. To prove this we can see if the lines for countries overlap with each other when applying our ratios of GDP per captia for each country and year.","metadata":{}},{"cell_type":"code","source":"gdp_per_capita_filtered_ratios_df_2[\"year\"] = gdp_per_capita_filtered_ratios_df_2[\"year\"].dt.year\ndef plot_adjust_country(df):\n    new_df = df.copy()\n    new_df[\"year\"] = new_df[\"date\"].dt.year\n    \n    for country in new_df[\"country\"].unique():\n        for year in new_df[\"year\"].unique():\n            new_df.loc[(new_df[\"country\"] == country) & (new_df[\"year\"] == year), \"num_sold\"] = new_df.loc[(new_df[\"country\"] == country) & (new_df[\"year\"] == year), \"num_sold\"] / gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"country\"] == country) & (gdp_per_capita_filtered_ratios_df_2[\"year\"] == year), \"ratio\"].values[0]\n            \n    plot_all(new_df)","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:27.310504Z","iopub.execute_input":"2025-01-04T21:25:27.310957Z","iopub.status.idle":"2025-01-04T21:25:27.323484Z","shell.execute_reply.started":"2025-01-04T21:25:27.310915Z","shell.execute_reply":"2025-01-04T21:25:27.322029Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_adjust_country(monthly_df)","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:27.325118Z","iopub.execute_input":"2025-01-04T21:25:27.325524Z","iopub.status.idle":"2025-01-04T21:25:30.769344Z","shell.execute_reply.started":"2025-01-04T21:25:27.325495Z","shell.execute_reply":"2025-01-04T21:25:30.767709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n\n- With the exception of Kenya (probably as the number of sales from Kenya are very low e.g. 5 sales a day and times by a very large constant) and sometimes Canada (because of the missing values) the number of sales overlap well for each store and product!\n\n**Insights:**\n- We can aggregate Countries when making the forecast and then disagregate the forecast by using the known ratios of GDP per capita for each year for each country.\n- We can use this information for imputation of the missing values (including the completely missing time series) by looking at sales from the same product and some store but for differnt countries, and applying the ratios to guess what the missing values would have been.","metadata":{}},{"cell_type":"markdown","source":"### Imputing","metadata":{}},{"cell_type":"code","source":"train_df_imputed = train_df.copy()\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")\n\ntrain_df_imputed[\"year\"] = train_df_imputed[\"date\"].dt.year\nfor year in train_df_imputed[\"year\"].unique():\n    # Impute Time Series 1 (Canada, Discount Stickers, Holographic Goose)\n    target_ratio = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Norway\"), \"ratio\"].values[0] # Using Norway as should have the best precision\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Canada\"), \"ratio\"].values[0]\n    ratio_can = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 2 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n\n    # Impute Time Series 3 (Only Missing Values)\n    current_ts =  train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Canada\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_can).values\n    \n    # Impute Time Series 4 (Kenya, Discount Stickers, Holographic Goose)\n    current_raito = gdp_per_capita_filtered_ratios_df_2.loc[(gdp_per_capita_filtered_ratios_df_2[\"year\"] == year) & (gdp_per_capita_filtered_ratios_df_2[\"country\"] == \"Kenya\"), \"ratio\"].values[0]\n    ratio_ken = current_raito / target_ratio\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Holographic Goose\")& (train_df_imputed[\"year\"] == year), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 5 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Premium Sticker Mart\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 6 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Stickers for Less\") & (train_df_imputed[\"product\"] == \"Holographic Goose\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n\n    # Impute Time Series 7 (Only Missing Values)\n    current_ts = train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year)]\n    missing_ts_dates = current_ts.loc[current_ts[\"num_sold\"].isna(), \"date\"]\n    train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Kenya\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] = (train_df_imputed.loc[(train_df_imputed[\"country\"] == \"Norway\") & (train_df_imputed[\"store\"] == \"Discount Stickers\") & (train_df_imputed[\"product\"] == \"Kerneler\") & (train_df_imputed[\"year\"] == year) & (train_df_imputed[\"date\"].isin(missing_ts_dates)), \"num_sold\"] * ratio_ken).values\n    \nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:30.771529Z","iopub.execute_input":"2025-01-04T21:25:30.772137Z","iopub.status.idle":"2025-01-04T21:25:37.681035Z","shell.execute_reply.started":"2025-01-04T21:25:30.772072Z","shell.execute_reply":"2025-01-04T21:25:37.679836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems a bit overkill to replace the entire timeseries for the remaining 2 missing values, I'll just fill them in manually using the graphs from earlier:","metadata":{}},{"cell_type":"code","source":"missing_rows = train_df_imputed.loc[train_df_imputed[\"num_sold\"].isna()]\ndisplay(missing_rows)\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 23719, \"num_sold\"] = 4\ntrain_df_imputed.loc[train_df_imputed[\"id\"] == 207003, \"num_sold\"] = 195\n\nprint(f\"Missing values remaining: {train_df_imputed['num_sold'].isna().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:37.682432Z","iopub.execute_input":"2025-01-04T21:25:37.682764Z","iopub.status.idle":"2025-01-04T21:25:37.708198Z","shell.execute_reply.started":"2025-01-04T21:25:37.682734Z","shell.execute_reply":"2025-01-04T21:25:37.706887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update monthly_df with our imputed data:\nweekly_df = train_df_imputed.groupby([\"country\",\"store\", \"product\", pd.Grouper(key=\"date\", freq=\"W\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()\nmonthly_df = train_df_imputed.groupby([\"country\",\"store\", \"product\", pd.Grouper(key=\"date\", freq=\"MS\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:37.710275Z","iopub.execute_input":"2025-01-04T21:25:37.711769Z","iopub.status.idle":"2025-01-04T21:25:37.879784Z","shell.execute_reply.started":"2025-01-04T21:25:37.711711Z","shell.execute_reply":"2025-01-04T21:25:37.878372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Store**","metadata":{}},{"cell_type":"markdown","source":"Lets test to see if the pattern between stores is the same, regardless of product or country.","metadata":{}},{"cell_type":"code","source":"store_weights = train_df_imputed.groupby(\"store\")[\"num_sold\"].sum()/train_df_imputed[\"num_sold\"].sum()\nstore_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:37.881507Z","iopub.execute_input":"2025-01-04T21:25:37.881848Z","iopub.status.idle":"2025-01-04T21:25:37.912601Z","shell.execute_reply.started":"2025-01-04T21:25:37.881818Z","shell.execute_reply":"2025-01-04T21:25:37.91122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"store_ratio_over_time = (train_df_imputed.groupby([\"date\",\"store\"])[\"num_sold\"].sum() / train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum()).reset_index()\nf,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = store_ratio_over_time, x=\"date\", y=\"num_sold\", hue=\"store\");\nax.set_ylabel(\"Proportion of sales\");","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:37.914371Z","iopub.execute_input":"2025-01-04T21:25:37.914811Z","iopub.status.idle":"2025-01-04T21:25:38.758149Z","shell.execute_reply.started":"2025-01-04T21:25:37.914767Z","shell.execute_reply":"2025-01-04T21:25:38.756755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_adjusted_store(df):\n    new_df = df.copy()\n    weights = store_weights.loc[\"Premium Sticker Mart\"] / store_weights\n    print(weights)\n    for store in weights.index:\n        new_df.loc[new_df[\"store\"] == store, \"num_sold\"] = new_df.loc[new_df[\"store\"] == store, \"num_sold\"] * weights[store]\n    plot_all(new_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:38.759673Z","iopub.execute_input":"2025-01-04T21:25:38.760061Z","iopub.status.idle":"2025-01-04T21:25:38.767181Z","shell.execute_reply.started":"2025-01-04T21:25:38.760029Z","shell.execute_reply":"2025-01-04T21:25:38.765674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If the lines between stores overlap perfectly then trend and seasonality are not unique to the store and we can ignore its effect.","metadata":{}},{"cell_type":"code","source":"plot_adjusted_store(monthly_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:38.769158Z","iopub.execute_input":"2025-01-04T21:25:38.76962Z","iopub.status.idle":"2025-01-04T21:25:42.09608Z","shell.execute_reply.started":"2025-01-04T21:25:38.769571Z","shell.execute_reply":"2025-01-04T21:25:42.094646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations:**\n- The dashed and solid lines representing the different stores overlap perfectly.","metadata":{}},{"cell_type":"markdown","source":"**Insight:**\n\n- This means we can perfectly predict the proportion of sales for each store, regardless of when it occurs.\n- Trend and seasonality are not unique to the store and we can ignore its effect. All differences in sales between stores can be explained by a single constant, which does not change over time.\n- This means we can forecast the store aggregated timeseries, and then disaggregating the forecasts based on historical proportions.","metadata":{}},{"cell_type":"markdown","source":"### **Product**","metadata":{}},{"cell_type":"markdown","source":"Product requires a different approach","metadata":{}},{"cell_type":"code","source":"product_df = train_df_imputed.groupby([\"date\",\"product\"])[\"num_sold\"].sum().reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:42.097672Z","iopub.execute_input":"2025-01-04T21:25:42.098067Z","iopub.status.idle":"2025-01-04T21:25:42.145574Z","shell.execute_reply.started":"2025-01-04T21:25:42.098035Z","shell.execute_reply":"2025-01-04T21:25:42.144302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data=product_df, x=\"date\", y=\"num_sold\", hue=\"product\");","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:42.151343Z","iopub.execute_input":"2025-01-04T21:25:42.151743Z","iopub.status.idle":"2025-01-04T21:25:43.13835Z","shell.execute_reply.started":"2025-01-04T21:25:42.151711Z","shell.execute_reply":"2025-01-04T21:25:43.136929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Product ratio for each date**","metadata":{}},{"cell_type":"markdown","source":"Lets have a look at the proportion of sales for each product each day:","metadata":{}},{"cell_type":"code","source":"product_ratio_df = product_df.pivot(index=\"date\", columns=\"product\", values=\"num_sold\")\nproduct_ratio_df = product_ratio_df.apply(lambda x: x/x.sum(),axis=1)\nproduct_ratio_df = product_ratio_df.stack().rename(\"ratios\").reset_index()\nproduct_ratio_df.head(4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:43.139791Z","iopub.execute_input":"2025-01-04T21:25:43.140178Z","iopub.status.idle":"2025-01-04T21:25:43.72517Z","shell.execute_reply.started":"2025-01-04T21:25:43.140146Z","shell.execute_reply":"2025-01-04T21:25:43.723981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = product_ratio_df, x=\"date\", y=\"ratios\", hue=\"product\");","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:43.726773Z","iopub.execute_input":"2025-01-04T21:25:43.727229Z","iopub.status.idle":"2025-01-04T21:25:44.756058Z","shell.execute_reply.started":"2025-01-04T21:25:43.727187Z","shell.execute_reply":"2025-01-04T21:25:44.754728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Observations**\n\nThe product ratio shows clear sinsidual lines for each product, with a period of 2 years.","metadata":{}},{"cell_type":"markdown","source":"**Insight**\n\nAs we have a clear seasonal pattern of the ratio of sales for each product, we do not need to forecast each product individually (or treat product as a categorical variable etc.). Instead we can forecast the sum of all sales each day, then afterwards convert the forecasted sum down to the forecast for each product, using the forecasted **ratios** for each date.","metadata":{}},{"cell_type":"markdown","source":"**Conclusions** \n\nAll this together means we only need to forecast 2 time series:\n1. The total sales each day\n2. The ratio in number of sales for each product each day\n\nWe still need to be careful about some timeseries where we might not have sales, or missing data.\n\n\nOnce we have completed the forecasts we can break the forecast down into the 3 categorical variables: Product, Country and Store.","metadata":{}},{"cell_type":"markdown","source":"## **Aggregated Time Series**","metadata":{}},{"cell_type":"markdown","source":"Lets take a look at the aggregated time series.","metadata":{}},{"cell_type":"code","source":"original_train_df_imputed = train_df_imputed.copy()\ntrain_df_imputed = train_df_imputed.groupby([\"date\"])[\"num_sold\"].sum().reset_index()","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:44.757935Z","iopub.execute_input":"2025-01-04T21:25:44.758379Z","iopub.status.idle":"2025-01-04T21:25:44.80421Z","shell.execute_reply.started":"2025-01-04T21:25:44.758342Z","shell.execute_reply":"2025-01-04T21:25:44.802954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = train_df_imputed, x=\"date\", y=\"num_sold\");","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:44.805839Z","iopub.execute_input":"2025-01-04T21:25:44.806389Z","iopub.status.idle":"2025-01-04T21:25:45.329098Z","shell.execute_reply.started":"2025-01-04T21:25:44.806348Z","shell.execute_reply":"2025-01-04T21:25:45.327687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is the time series we need to forecast.","metadata":{}},{"cell_type":"code","source":"weekly_df = train_df.groupby([pd.Grouper(key=\"date\", freq=\"W\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()\nmonthly_df = train_df.groupby([pd.Grouper(key=\"date\", freq=\"MS\")])[\"num_sold\"].sum().rename(\"num_sold\").reset_index()","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:45.330923Z","iopub.execute_input":"2025-01-04T21:25:45.3314Z","iopub.status.idle":"2025-01-04T21:25:45.364815Z","shell.execute_reply.started":"2025-01-04T21:25:45.33136Z","shell.execute_reply":"2025-01-04T21:25:45.363423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,9))\nsns.lineplot(data=monthly_df, x=\"date\", y=\"num_sold\");","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:45.366674Z","iopub.execute_input":"2025-01-04T21:25:45.367203Z","iopub.status.idle":"2025-01-04T21:25:45.850478Z","shell.execute_reply.started":"2025-01-04T21:25:45.367154Z","shell.execute_reply":"2025-01-04T21:25:45.849125Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,9))\nsns.lineplot(data=weekly_df[1:-1], x=\"date\", y=\"num_sold\");","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:45.852467Z","iopub.execute_input":"2025-01-04T21:25:45.852998Z","iopub.status.idle":"2025-01-04T21:25:46.25205Z","shell.execute_reply.started":"2025-01-04T21:25:45.852956Z","shell.execute_reply":"2025-01-04T21:25:46.250528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Seasonality**","metadata":{}},{"cell_type":"code","source":"def plot_seasonality(df, x_axis):\n    \n\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n    df[\"day_of_year\"] = df['date'].apply(\n        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n    )\n\n    f,ax = plt.subplots(1,1,figsize=(20,8))\n    sns.lineplot(data=df, x=x_axis, y=\"num_sold\", ax=ax);\n    ax.set_title(\"{} Seasonality\".format(x_axis))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:46.253674Z","iopub.execute_input":"2025-01-04T21:25:46.25406Z","iopub.status.idle":"2025-01-04T21:25:46.261964Z","shell.execute_reply.started":"2025-01-04T21:25:46.254028Z","shell.execute_reply":"2025-01-04T21:25:46.260469Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_seasonality(train_df_imputed, \"month\")","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:46.263718Z","iopub.execute_input":"2025-01-04T21:25:46.264161Z","iopub.status.idle":"2025-01-04T21:25:46.856798Z","shell.execute_reply.started":"2025-01-04T21:25:46.264109Z","shell.execute_reply":"2025-01-04T21:25:46.85553Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_seasonality(train_df_imputed, \"day_of_week\")","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:46.858296Z","iopub.execute_input":"2025-01-04T21:25:46.85861Z","iopub.status.idle":"2025-01-04T21:25:47.306331Z","shell.execute_reply.started":"2025-01-04T21:25:46.858569Z","shell.execute_reply":"2025-01-04T21:25:47.305018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_seasonality(train_df_imputed, \"day_of_year\")","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:47.308171Z","iopub.execute_input":"2025-01-04T21:25:47.309178Z","iopub.status.idle":"2025-01-04T21:25:54.406616Z","shell.execute_reply.started":"2025-01-04T21:25:47.30913Z","shell.execute_reply":"2025-01-04T21:25:54.405286Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Modeling**","metadata":{}},{"cell_type":"markdown","source":"We required 2 forecasts:\n\n1. **Total Sales** Forecast\n2. **Product Sales Ratio** Forecast","metadata":{}},{"cell_type":"markdown","source":"## **Total Sales Forecast**","metadata":{}},{"cell_type":"markdown","source":"Lets revist the graph of sales we wish to forecast:","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = train_df_imputed, x=\"date\", y=\"num_sold\");","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:54.408419Z","iopub.execute_input":"2025-01-04T21:25:54.408873Z","iopub.status.idle":"2025-01-04T21:25:54.937239Z","shell.execute_reply.started":"2025-01-04T21:25:54.408828Z","shell.execute_reply":"2025-01-04T21:25:54.935958Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#get the dates to forecast for\ntest_total_sales_df = test_df.groupby([\"date\"])[\"id\"].first().reset_index().drop(columns=\"id\")\n#keep dates for later\ntest_total_sales_dates = test_total_sales_df[[\"date\"]]","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:54.939095Z","iopub.execute_input":"2025-01-04T21:25:54.93951Z","iopub.status.idle":"2025-01-04T21:25:54.953989Z","shell.execute_reply.started":"2025-01-04T21:25:54.93947Z","shell.execute_reply":"2025-01-04T21:25:54.952332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lets do a bit of feature engineering. There's probably room for improvement here:","metadata":{}},{"cell_type":"code","source":"def feature_engineer(df):\n    new_df = df.copy()\n    new_df[\"month\"] = df[\"date\"].dt.month\n    new_df[\"month_sin\"] = np.sin(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"month_cos\"] = np.cos(new_df['month'] * (2 * np.pi / 12))\n    new_df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n    new_df[\"day_of_week\"] = new_df[\"day_of_week\"].apply(lambda x: 0 if x<=3 else(1 if x==4 else (2 if x==5 else (3))))\n    \n    new_df[\"day_of_year\"] = df['date'].apply(\n        lambda x: x.timetuple().tm_yday if not (x.is_leap_year and x.month > 2) else x.timetuple().tm_yday - 1\n    )\n    new_df['day_sin'] = np.sin(new_df['day_of_year'] * (2 * np.pi /  365.0))\n    new_df['day_cos'] = np.cos(new_df['day_of_year'] * (2 * np.pi /  365.0))\n\n    #new_df['week_of_year'] = new_df['date'].dt.isocalendar().week\n    \n    new_df[\"important_dates\"] = new_df[\"day_of_year\"].apply(lambda x: x if x in [1,2,3,4,5,6,7,8,9,10,99, 100, 101, 125,126,355,256,357,358,359,360,361,362,363,364,365] else 0)\n    #new_df[\"year\"] = df[\"date\"].dt.year - 2010\n    \n    new_df = new_df.drop(columns=[\"date\",\"month\",\"day_of_year\"])\n    new_df = pd.get_dummies(new_df, columns = [\"important_dates\",\"day_of_week\"], drop_first=True)\n    \n    return new_df","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:54.955885Z","iopub.execute_input":"2025-01-04T21:25:54.956375Z","iopub.status.idle":"2025-01-04T21:25:54.969298Z","shell.execute_reply.started":"2025-01-04T21:25:54.956334Z","shell.execute_reply":"2025-01-04T21:25:54.967876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_total_sales_df = feature_engineer(train_df_imputed)\ntest_total_sales_df = feature_engineer(test_total_sales_df)","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:54.971084Z","iopub.execute_input":"2025-01-04T21:25:54.971505Z","iopub.status.idle":"2025-01-04T21:25:55.430259Z","shell.execute_reply.started":"2025-01-04T21:25:54.971475Z","shell.execute_reply":"2025-01-04T21:25:55.428949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(train_total_sales_df.head(2))\ndisplay(test_total_sales_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:55.432024Z","iopub.execute_input":"2025-01-04T21:25:55.43246Z","iopub.status.idle":"2025-01-04T21:25:55.469326Z","shell.execute_reply.started":"2025-01-04T21:25:55.432419Z","shell.execute_reply":"2025-01-04T21:25:55.467985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = train_total_sales_df[\"num_sold\"]\nX = train_total_sales_df.drop(columns=\"num_sold\")\nX_test = test_total_sales_df","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:55.470973Z","iopub.execute_input":"2025-01-04T21:25:55.47133Z","iopub.status.idle":"2025-01-04T21:25:55.479719Z","shell.execute_reply.started":"2025-01-04T21:25:55.471303Z","shell.execute_reply":"2025-01-04T21:25:55.478492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define and fit the model, then make the forecast.","metadata":{}},{"cell_type":"code","source":"model = Ridge(tol=1e-2, max_iter=1000000, random_state=0)\nmodel.fit(X, y)\npreds = model.predict(X_test)\ntest_total_sales_dates[\"num_sold\"] = preds ","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:38:56.208768Z","iopub.execute_input":"2025-01-04T21:38:56.209314Z","iopub.status.idle":"2025-01-04T21:38:56.228588Z","shell.execute_reply.started":"2025-01-04T21:38:56.209277Z","shell.execute_reply":"2025-01-04T21:38:56.226719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualising the forecast:","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data = pd.concat([train_df_imputed,test_total_sales_dates]).reset_index(drop=True), x=\"date\", y=\"num_sold\", linewidth=0.6);","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:38:56.927675Z","iopub.execute_input":"2025-01-04T21:38:56.928128Z","iopub.status.idle":"2025-01-04T21:38:57.560455Z","shell.execute_reply.started":"2025-01-04T21:38:56.928097Z","shell.execute_reply":"2025-01-04T21:38:57.55901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The forecast looks good, although we don't know whether the total number of sales will be more similar to 2011, 2012, 2013 and 2014, or more similar to 2010, 2015. 2016 or neither.","metadata":{}},{"cell_type":"markdown","source":"## **Product Ratio Forecast**","metadata":{}},{"cell_type":"markdown","source":"We now need to forecast the sales ratio between products for 2017, 2018 and 2019.","metadata":{}},{"cell_type":"markdown","source":"The period of the product ratio sinsidual curves appear to be 2 years. So to forecast 2017 and 2019 I use the 2015 data and to forecast 2018 I use the 2014 data.","metadata":{}},{"cell_type":"code","source":"product_ratio_2017_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\nproduct_ratio_2018_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2016].copy()\nproduct_ratio_2019_df = product_ratio_df.loc[product_ratio_df[\"date\"].dt.year == 2015].copy()\n\nproduct_ratio_2017_df[\"date\"] = product_ratio_2017_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2018_df[\"date\"] = product_ratio_2018_df[\"date\"] + pd.DateOffset(years=2)\nproduct_ratio_2019_df[\"date\"] =  product_ratio_2019_df[\"date\"] + pd.DateOffset(years=4)\n\nforecasted_ratios_df = pd.concat([product_ratio_2017_df, product_ratio_2018_df, product_ratio_2019_df])","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:56.202667Z","iopub.execute_input":"2025-01-04T21:25:56.203105Z","iopub.status.idle":"2025-01-04T21:25:56.225207Z","shell.execute_reply.started":"2025-01-04T21:25:56.203068Z","shell.execute_reply":"2025-01-04T21:25:56.223989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualising the forecast:","metadata":{}},{"cell_type":"code","source":"temp_df = pd.concat([product_ratio_df,forecasted_ratios_df]).reset_index(drop=True)\nf,ax = plt.subplots(figsize=(20,10))\nsns.lineplot(data=temp_df, x=\"date\", y=\"ratios\", hue=\"product\");\nax.axvline(pd.to_datetime(\"2017-01-01\"), color='black', linestyle='--');","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:56.22664Z","iopub.execute_input":"2025-01-04T21:25:56.226974Z","iopub.status.idle":"2025-01-04T21:25:57.522536Z","shell.execute_reply.started":"2025-01-04T21:25:56.226938Z","shell.execute_reply":"2025-01-04T21:25:57.521169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Disaggregating Total Sales Forecast**","metadata":{}},{"cell_type":"markdown","source":"Now we have our two required forecasts and ratios, we need to divide the total sales forecast for each day between the categorical variables so we get the forecast for each day, country, product and store.","metadata":{}},{"cell_type":"code","source":"# Adding in the store ratios\nstore_weights_df = store_weights.reset_index()\ntest_sub_df = pd.merge(test_df, test_total_sales_dates, how=\"left\", on=\"date\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"day_num_sold\"})\n# Adding in the product ratios\ntest_sub_df = pd.merge(test_sub_df, store_weights_df, how=\"left\", on=\"store\")\ntest_sub_df = test_sub_df.rename(columns = {\"num_sold\":\"store_ratio\"})\n# Adding in the country ratios\ntest_sub_df[\"year\"] = test_sub_df[\"date\"].dt.year\ntest_sub_df = pd.merge(test_sub_df, gdp_per_capita_filtered_ratios_df_2, how=\"left\", on=[\"year\", \"country\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratio\":\"country_ratio\"})\n# Adding in the product ratio\ntest_sub_df = pd.merge(test_sub_df, forecasted_ratios_df, how=\"left\", on=[\"date\", \"product\"])\ntest_sub_df = test_sub_df.rename(columns = {\"ratios\":\"product_ratio\"})\n\n# Disaggregating the forecast\ntest_sub_df[\"num_sold\"] = test_sub_df[\"day_num_sold\"] * test_sub_df[\"store_ratio\"] * test_sub_df[\"country_ratio\"] * test_sub_df[\"product_ratio\"]\ntest_sub_df[\"num_sold\"] = test_sub_df[\"num_sold\"].round()\ndisplay(test_sub_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2025-01-04T21:25:57.524668Z","iopub.execute_input":"2025-01-04T21:25:57.525172Z","iopub.status.idle":"2025-01-04T21:25:57.675756Z","shell.execute_reply.started":"2025-01-04T21:25:57.525128Z","shell.execute_reply":"2025-01-04T21:25:57.674473Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lets have a look at all 90 forecasts to see if each individual forecast looks reasonable:","metadata":{}},{"cell_type":"code","source":"def plot_individual_ts(df):\n    colour_map = {\"Canada\": \"blue\", \"Finland\": \"orange\", \"Italy\": \"green\", \"Kenya\":\"red\", \"Norway\": \"purple\", \"Singapore\": \"brown\"}\n    for country in df[\"country\"].unique():\n        f,axes = plt.subplots(df[\"store\"].nunique()*df[\"product\"].nunique(),figsize=(20,75))\n        count = 0\n        for store in df[\"store\"].unique():\n            for product in df[\"product\"].unique():\n                plot_df = df.loc[(df[\"product\"] == product) & (df[\"country\"] == country) & (df[\"store\"] == store)]\n                sns.lineplot(data = plot_df, x= \"date\", y=\"num_sold\", linewidth=0.5, ax=axes[count], color=colour_map[country])\n                axes[count].set_title(f\"{country} - {store} - {product}\")\n                axes[count].axvline(pd.to_datetime(\"2017-01-01\"), color='black', linestyle='--');\n                count+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:57.677387Z","iopub.execute_input":"2025-01-04T21:25:57.677764Z","iopub.status.idle":"2025-01-04T21:25:57.688537Z","shell.execute_reply.started":"2025-01-04T21:25:57.67773Z","shell.execute_reply":"2025-01-04T21:25:57.687056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_individual_ts(pd.concat([original_train_df_imputed,test_sub_df]).reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T21:25:57.690401Z","iopub.execute_input":"2025-01-04T21:25:57.690758Z","iopub.status.idle":"2025-01-04T21:27:01.445738Z","shell.execute_reply.started":"2025-01-04T21:25:57.690728Z","shell.execute_reply":"2025-01-04T21:27:01.444261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/playground-series-s5e1/sample_submission.csv\")\nsubmission[\"num_sold\"] = test_sub_df[\"num_sold\"]\ndisplay(submission.head(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:04:47.534689Z","iopub.execute_input":"2025-01-04T02:04:47.535047Z","iopub.status.idle":"2025-01-04T02:04:47.584374Z","shell.execute_reply.started":"2025-01-04T02:04:47.535016Z","shell.execute_reply":"2025-01-04T02:04:47.583215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2025-01-02T21:03:04.511369Z","iopub.execute_input":"2025-01-02T21:03:04.511867Z","iopub.status.idle":"2025-01-02T21:03:04.694329Z","shell.execute_reply.started":"2025-01-02T21:03:04.511824Z","shell.execute_reply":"2025-01-02T21:03:04.693165Z"},"trusted":true},"outputs":[],"execution_count":null}]}